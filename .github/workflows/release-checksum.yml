name: Extract Installer Info from GitHub Release

on:
  release:
    types: [published, edited, deleted]
  workflow_dispatch:  # Just to show this workflow in the Actions tab

jobs:
  extract-and-generate:
    # Only run on release events, skip manual triggers
    if: github.event_name != 'workflow_dispatch' && github.event.release.draft == false
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Cache ".cache" directory (node_modules & downloads)
        uses: actions/cache@v4
        with:
          path: .cache
          key: ${{ runner.os }}-cache-${{ hashFiles('.github/workflows/**.yml') }}
          restore-keys: |
            ${{ runner.os }}-cache-

      - name: Install dependencies
        run: npm install undici@6 --prefix .cache

      - name: Extract links, download files, and add/update JSON
        env:
          GITHUB_EVENT_PATH: ${{ github.event_path }}
        run: |
          node <<'EOF'
          const fs = require('fs');
          const path = require('path');
          const crypto = require('crypto');
          const { request } = require(path.resolve('.cache/node_modules/undici'));

          const EVENT_PATH = process.env.GITHUB_EVENT_PATH;
          const STABLE_FILE = 'releases.json';
          const PRE_FILE = 'releases-pre.json';

          // Match .exe or .msi from [name.exe](url) or [file name (.msi)](url)
          const assetRegex = /[:-] \[([^\]]+\.(?:exe|msi)(?:[^\]]+)?)\]\((https?:\/\/[^\)]+)\)/gi;

          function hashContent(content, algo = 'sha256') {
            return crypto.createHash(algo).update(content).digest('hex');
          }

          function getSafeFilename(name, url) {
            try {
              return encodeURIComponent(name);
            } catch {
              return encodeURIComponent(path.basename(url));
            }
          }

          async function downloadBuffer(url, maxRetries = 3) {
            const delay = ms => new Promise(res => setTimeout(res, ms));

            for (let attempt = 1; attempt <= maxRetries; attempt++) {
              try {
                const { body, statusCode } = await request(url);
                if (statusCode !== 200) {
                  throw new Error(`HTTP ${statusCode}`);
                }
                const chunks = [];
                for await (const chunk of body) {
                  chunks.push(chunk);
                }
                return Buffer.concat(chunks);
              } catch (err) {
                console.warn(`âš ï¸ Attempt ${attempt} failed for ${url}: ${err.message}`);
                if (attempt < maxRetries) {
                  await delay(1000 * attempt);
                } else {
                  console.error(`âŒ All ${maxRetries} attempts failed for ${url}`);
                  return null;
                }
              }
            }
          }

          async function getOrCacheAsset(descDigest, name, url) {
            const downloadDir = path.join('.cache', 'downloads', descDigest);
            fs.mkdirSync(downloadDir, { recursive: true });

            const safeName = getSafeFilename(name, url);
            const cachePath = path.join(downloadDir, safeName);
            let buffer;

            if (fs.existsSync(cachePath)) {
              buffer = fs.readFileSync(cachePath);
              console.log(`â™»ï¸ Reused cached asset: ${cachePath}`);
            } else {
              try {
                buffer = await downloadBuffer(url);
                fs.writeFileSync(cachePath, buffer);
                console.log(`â¬‡ï¸ Downloaded and cached: ${cachePath}`);
              } catch (err) {
                console.error(`âŒ Failed to fetch ${url} for ${name}: ${err.message}`);
                return null;
              }
            }

            return {
              name,
              url,
              size: buffer.length,
              digest: {
                sha256: hashContent(buffer),
                sha1: hashContent(buffer, 'sha1'),
                md5: hashContent(buffer, 'md5')
              }
            };
          }

          async function getAssetInfoFromBody(desc, descDigest) {
            const matches = [...desc.matchAll(assetRegex)];
            const assets = [];

            for (const [, name, url] of matches) {
              try {
                const asset = await getOrCacheAsset(descDigest, name, url);
                if (asset) {
                  assets.push(asset);
                }
              } catch (err) {
                console.error(`âŒ Error processing asset ${name}: ${err.message}`);
              }
            }

            return assets;
          }
          async function processRelease(release, action) {
            const file = release.prerelease ? PRE_FILE : STABLE_FILE;
            const fileExists = fs.existsSync(file);
            const existing = fileExists ? JSON.parse(fs.readFileSync(file, 'utf8')) : [];
            const version = release.tag_name.replace(/^v/i, '');

            if (action === 'deleted') {
              if (!fileExists) {
                console.log(`â„¹ï¸ ${file} does not exist, nothing to remove`);
                return;
              }

              const updated = existing.filter(e => e.version !== version);
              if (updated.length === existing.length) {
                console.log(`â„¹ï¸ Version ${version} not found in ${file}, nothing to remove`);
                return;
              }

              fs.writeFileSync(file, JSON.stringify(updated, null, 2));
              console.log(`ðŸ—‘ï¸ Removed version ${version} from ${file}`);
              return;
            }

            const desc = release.body || '';
            const descDigest = hashContent(desc);
            const fullDigest = 'sha256:' + descDigest;

            const existingIndex = existing.findIndex(e => e.version === version);
            const prevDigest = existingIndex !== -1 ? existing[existingIndex].desc_digest : null;

            // If the release is already in the file and the `desc_digest` hasn't changed
            if (existingIndex !== -1 && prevDigest === fullDigest) {
              console.log(`â†©ï¸ Skipping unchanged version ${version}`);
              return;
            }

            const assets = await getAssetInfoFromBody(desc, descDigest);
            if (!assets.length) {
              console.warn(`âš ï¸ No valid assets found in release ${version}`);
            }

            const entry = {
              version,
              url: release.html_url,
              desc_digest: fullDigest,
              assets
            };

            const updated = [...existing];  // clone
            if (existingIndex === -1) {
              console.log(`âž• Adding new version ${version}`);
              updated.unshift(entry); // Add to top
            } else {
              console.log(`ðŸ”„ Updating modified version ${version}`);
              updated[existingIndex] = entry;
            }

            const newJson = JSON.stringify(updated, null, 2);

            // Final check, prevents unnecessary commit when thereâ€™s no actual data change
            if (fileExists && fs.readFileSync(file, 'utf8') === newJson) {
              console.log(`â†©ï¸ ${file} unchanged, not rewritten.`);
              return;
            }

            fs.writeFileSync(file, newJson);
            console.log(`âœ… ${fileExists ? 'Updated' : 'Created'} ${file} with ${updated.length} release(s)`);
          }

          async function main() {
            const event = JSON.parse(fs.readFileSync(EVENT_PATH, 'utf8'));
            const release = event.release;

            if (!release || !release.tag_name) {
              throw new Error("No release info found in event payload.");
            }

            await processRelease(release, event.action);
          }

          main().catch(err => {
            console.error(err);
            process.exit(1);
          });
          EOF

      - name: Check for newly created or updated release JSON files
        id: commitinfo
        run: |
          shopt -s nullglob
          files=(releases*.json)
          git add "${files[@]}"

          # Default message
          echo "message=ðŸ¤– Update release info for ${{ github.ref_name }}" >> "$GITHUB_OUTPUT"

          # Check if any files are newly added
          files_added=$(git diff --cached --name-status -- releases*.json | grep '^A' | wc -l)
          if [ "$files_added" -gt 0 ]; then
            echo "message=ðŸ¤– Initial release info for ${{ github.ref_name }}" >> "$GITHUB_OUTPUT"
          fi

          # Check if any lines were removed (e.g. deleted release entry)
          files_removed=$(git diff --cached --numstat -- releases*.json | awk '$2 > $1 {exit 1}' && echo 0 || echo 1)
          if [ "$files_removed" -eq 1 ]; then
            echo "message=chore: sync release JSON files (remove old version)" >> "$GITHUB_OUTPUT"
          fi

      - name: Commit release info JSON
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: ${{ steps.commitinfo.outputs.message }}
          file_pattern: releases*.json
